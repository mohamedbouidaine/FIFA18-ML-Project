{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "767WJjXxFjGa"
   },
   "source": [
    "# 1. Problem to solve:\n",
    "### The problem to solve is the following:\n",
    "- Based on all given players attributes, predict the preferred position of the player.\n",
    "\n",
    "- For this, dataset will be cleaned by deleting entries that has null values. Also, some attributes are like this : 70+9, entries having values like this will be replaced by the evaluated value. Ex. 70+9 -> 79.\n",
    "\n",
    "- Some players have two preferred positions, to facilitate classification and to not have a combination of too many positions, only the first position will be kept.\n",
    "\n",
    "- Some positions will be reduced and merged to another similar positoins to reduce the number of positions:\n",
    "  - LWB -> LB.\n",
    "  - RWB -> RB\n",
    "  - CF -> ST\n",
    "  - CAM and CDM -> CM.\n",
    "\n",
    "# 2. Machine learning models:\n",
    "- Three machine learning models will be built to solve our problem, Naive Bayes, Random Forests and SVM.\n",
    "- Dataset will be splitted with 20% for test, and 80% for train.\n",
    "- For each algorithm, different values of hyperparametres will be defined. Then, a grid-search algorithm will be used to select the best parameters. Also, our problem is useful to make predictions for unknown or new players, cross validation algorithm will be used alongside with gridsearch to validate that the model is more general.\n",
    "\n",
    "# 3. Performance evaluation\n",
    "Different evaluation metrics are used to evaluate results and to compare the performance of the three models.\n",
    "The most important metric in our case will be F1-score. That's because our classes are not balanced (they don't have the same number of population), and the F1-score considers well this case, in contrast to accuracy for example, that only cares about correct predictions and might favor classes with more population.\n",
    "\n",
    "# 4. Discussion on the Performances of the Algorithms:\n",
    "\n",
    "- Limitations of the Approach:**  \n",
    "  - Merging positions reduces complexity but may result in losing subtle positional nuances, like LWB vs. LB.  \n",
    "  - The imbalance in dataset classes likely influenced Naive Bayes' performance, even with metrics such as F1-Score.  \n",
    "  - Restricting multi-preferred positions to the first entry could introduce bias, underrepresenting alternative roles.  \n",
    "\n",
    "- Performance and Observations from Results:\n",
    "    - Naive Bayes:  \n",
    "      - Performed noticeably worse across all metrics (F1-Score, Recall, Precision, Accuracy) due to its reliance on the independence assumption, which limits its ability to model complex interactions between features.  \n",
    "  - Random Forests:  \n",
    "      - Achieved the best overall performance, showing a balance between Precision and Recall, leading to the highest F1-Score. The robust performance highlights its strength in handling non-linear relationships and imbalanced datasets, especially with well-tuned `n_estimators` and `max_depth`.  \n",
    "  - SVM: \n",
    "      - Comparable to Random Forests in most metrics, performing slightly better in Precision and Accuracy. However, its computational complexity could be a limitation for larger datasets. The results validate the effectiveness of hyperparameter tuning (e.g., `C` and kernel type).  \n",
    "\n",
    "- General Observations:  \n",
    "  - Random Forest and SVM demonstrated robust performance, especially with imbalanced data, showing their suitability for this classification task.  \n",
    "  - Naive Bayes, while computationally efficient, is not as reliable for datasets with interdependent attributes and class imbalances.  \n",
    "  - Hyperparameter tuning via grid search improved Random Forests and SVM results, but it required significant computational resources.  \n",
    "\n",
    "The visualized metrics confirm the superiority of Random Forests and SVM for predicting player positions accurately and consistently."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
